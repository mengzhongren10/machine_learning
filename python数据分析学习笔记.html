Python包Pandas学习笔记

1 package的载入

  import numpy as np
  import pandas as pd

2 数据读取(“/”暂作“或”使用)
    # \s+是利用空白符进行分割,可使用正则表达式匹配
    # header可以制定列明对应的行数，如果没有就是None
    # index_col就是行名对应的列数，如果没有也是None
    # parse_dates日期型列
    pd.read_csv(path, sep = '\s+'/'\t'/',', header=0/None, usecols=[...], 
	,parse_dates=[0]/['Date'], index_col=0/None, encoding='utf-8' )

3 数据输出
    # na_rep是填充的缺失值
    # header可以给一个list用作数据输出的列名
    # columns可以给定数字list用于选取输出特定的列
    pd.to_csv(path, sep='', na_rep='NA', header=True/['xx','xx'], index=True, 
	columns=[1,2,3..], encoding='utf-8')

4 创建 DataFrame，Series对象
    
    # DataFrame和Series都接受list和numpy array数据，若要转换回array可以用df.values或S.values
    # DataFrame还可以接受dictionary对象
    df = pd.DataFrame(np.random.rand(10,5),columns=list('ABCDE')) #列名为ABCDE
    S = pd.Series([1,2,3,4])

5 数据类型总结

    df = pd.DataFrame({'string':list('abc'),
                       'int64':list(range(1,4)),
                       'uint8': np.arange(3, 6).astype('u1'),
                       'float64': np.arange(4.0, 7.0),
                       'bool1': [True, False, True],
                       'dates': pd.date_range('now', periods=3).values,
                       'category': pd.Series(list("ABC")).astype('category')
                            })
    

    S.astype(np.int64) # 数据类型转化
    S.dtype == np.int64 # 数据类型判断
    S.to_frame() # 将Series数据转换为DataFrame数据
    pd.to_datetime(df.a) # 将df数据a列转换为datetime类型
    pd.to_numeric(aDF.loc[:, 'xxx'])  #转换成数字
    df.apply(lambda x:x)     #将groupby 对象转成DataFrame类型

6 查看数据

    df.head(10) # 查看数据前10行
    df.tail(10) # 查看数据后10行
    df.shape    # 数据对应的行列数
    df.info()   # 数据索引，类型，内存信息
    df.describe(include = 'all') # 对于数字型数据进行分位数统计,all代表对所有数据类型统计
    S.value_counts(dropna=False) # 对Series里面的值进行个数统计，NA也会统计
    df.apply(pd.Series.value_counts) #返回每个值在各列中的个数，没有则用NaN代替
    df.index # 返回数据的索引
    df.columns # 返回数据的列名
    df.mean()/min()/median()/std()/count() # 分别是df的列均值,最小值,中位数,标准差,非空值
    df.corr() # 列之间的相关系数
    df.idxmax(0) # 每列最大数对应行的index的名
    df.idxmax(1) # 每行最大数对应的列名
    aDF.nsmallest(columns='age', n=20) # 取出年龄最小的20个数据
    S.is_unique # 确定Series数据是否是unique的, 返回bool值
    df.xxx.unique() # 查看某列中有多少不同的取值
    df.xxx.nunique()  # 查看不同取值的数量
    df[df['Rating'].isin([19])]  # 查看某一列的值为‘19’的记录
    df[df['Content Rating'].values == 'Mature 17+'] # 筛选出某一列的值为‘Mature 17+’的记录
    df[df['xxx'] > 5000000]      # 筛选出某一列的数值大于某个数的记录
    df.unstack()      # 将数据的行“旋转”为列
    df.stack()        # 将数据的列“旋转”为行
    

    数值型变量
    # describe函数查看部分变量的分布
    df.describe()
  
    分类变量
    df.describe(include=[np.object])
    # count: 非缺失值的个数
    # unique: 非重复值得个数
    # top: 最高频值
    # freq: 最高频值出现次数
    
7  数据的截取
    df.xxx / df['xxx'] #返回数据的某一列，列名为xxx，类型为Series
    df[['xx','yy']] #选取多列数据，数据类型为DataFrame
    df.iloc[3] #选取第4行数据
    df.iloc[[1,2,3],:] #选取多行数据
    df.loc[['x','y','z'],['a','b','c']] #根据行index名和列名进行数据截取
    df.ix[..,..] # ix是用行名列名以及行数列数混合赋值的情况下数据的截取
    df[(df.col>0.5) & (df.col<10)] # 筛选col大于0.5小于10的行返回
    aDF.loc[(aDF['a']>10) & (aDF['b']<100), :] # 也可以给条件进行筛选,& | ~进行逻辑运算
    df.values #返回df对应的numpy array值
    df.values[10][5] #求df数据11行6列的值
    df[df['xxx'].notnull(), :] = 10 # 对空值赋值	?
    S.str.startswith('G')   #Seriers以G开头的字符, 返回bool值
    S != 'xxx'              # Series中不为xxx的位置, 返回bool值
    S.isin(['a','b','c'])   # 返回bool值如果S在list['a','b','c']中, 返回true

8 数据清洗
    del df['a'] #删除数据df的a列
    df.drop(['B','C'],axis=1,inplace=True)  # 删除数据df的B,C列, 在原数据上进行修改
    df.dropna(how = 'all',axis = 0)   #对行进行操作,如果一行里面全都是na那么就删除,如果要一列里面全是na就删除,那么axis=1
    df.loc[:, 'newcol'] = 2000        # 如果没有newcol那么就新加一列
    df['newcol'] = 2000               # df新添加一列
    df.columns = ['a','b']      # 更改数据的列名
    df.isnull().sum()           # 统计各列中缺失值的个数
    df.notnull().sum()          # 统计各列中非缺失值的个数
    df.shape[0] - df.count()    # 统计各列中非缺失值得个数
    df[df.isnull().values==True]  # 筛选出缺失的部分数据	
    
    df.drop_duplicates([...], 'first'/'last'/False)   # 移除重复项, list可以是列名也可以是数字序列,first是保留重复中的第一个, last是保留重复中的最后一个, False一个不留, 只要重复都删除
    df.dropna(axis=0/1, how='any'/'all', thresh=n)   # 移除数据中的缺失行,axis0是行删除,1是列删除
                                                     # any是只要有一个就删除,all是所有的都是才删除
                                                     # thresh目的是大于n个NA才删除
    S.fillna(x)      #对缺失值进行填充，填充值为x
    aDF.loc[:,'xx'].fillna(aSeires)    # 对数据进行填充,根据aSeires的index和aDF的index进行填充				    
    S.astype(np.float) # 数据的类型转换
    S.replace(1, 'one') # 将1替换为'one'
    S.replace([1,2], ['one','two']) # 将1替换为one, 2替换为two
    df.rename(index/columns={'old1':'new1','old2':'new2'}) # 修改行名列名，将old1改为new1，将old2改为new2
    df.set_index('B') # 修改index，将B所在的列作为行索引
    df.sort_index() # 将数据按照index进行排序
    df.sort_values([col1, col2], ascending=[True,False]) # 根据col1和col2的值进行排序，col1是升序，col2是降序
    S.argsort() # 返回Series对应的值的order，S[S.argsort()]返回的是对应的从小到大的Series数值			    
    df.reset_index(drop=False/True, inplace=False/True) # 数据的index换成从0开始的, drop是说是否保留原来的index, 保留的话就多一列, inplace是说是否修改原来的df
    data['Age'] = pd.cut(data['Age'], bins=6, labels=np.arange(6)) # 对数值型数据进行区间分割，分割成6个bin，label用0-5表示
    np.tile(a, N).flatten() # 对数据a进行重复

9  数据分组
    df.groupby(col).size() # 按照col对数据进行分组，并计算每一组的数量, 如果是count的话每列都会计算一次分组后的数量, 比较冗余					    
    df.groupby([col1, col2]).mean() # 按照col1,col2进行分组，并计算各组的均值				    
    df.groupby([col1, col2]).agg(['min', 'max', 'mean']) # 按照col1col2进行分组，计算各组之间的min, max, mean				    
    df.groupby([col1, col2]).agg({'a':'min', 'b':'max', 'c':'mean'}) # 按照col1col2进行分组，计算各组之间的a列的min, b列的max, c列的mean				    
    df.groupby(col1)[col2].mean() # 计算按照col1分组的col2对应的均值
    df.pivot.table(index=col1, values=[col2, col3], aggfun='mean') # 以col1的值为index,以col2,col3值为列进行分组计算各元素平均值					    
    df.apply(np.mean) # 计算每一列的平均值
    df.apply(np.max, axis=1) #计算每一行的平均值
    df.applymap(lambda x : x.upper()) # 对多列数据操作, 这里是对各列都大写
    for name,data in df.groupby('col'):
	     print name # col列分类后的值
	     print data # col列名称等于name对应的数据行
    df.groupby('name').apply(ownfunc) # 可以对不同组进行自定义函数操作
    df.apply(ownfunc)   #可以对不同列数据进行自定义函数操作
					    
10  数据合并
    df1.append([df2, df3])  # 也可以追加两个DF
    pd.concat([df1.set_index('a'), df2.set_index('a')], sort=False, axis=1, join='inner') # 和上述利用merge在a字段上进行内连接的效果类似,因为concat是基于index进行连接的,merge可以不基于index,指定字段					    
    pd.concat([df1, df2], axis=1) #列数相同, 行合在一起
    pd.concat(frames, keys=['x', 'y', 'z'], axis=0) #行数相同, 列合在一起, 每个数据的来源分别标注xyz
    pd.merge(df1, df2, on=['key1','key2'], how='outer'/'inner'/'left'/'right') # 合并df1和df2,根据df1的key1和df2的key2, 连接方式是外链接..				    
    pd.merge(df1, df2, how='inner', left_index=True, right_on='id') # 对数据进行merge,左表以index作为连接关键字,右表用id作为关键字					    
    np.vstack((a,b)) # 乱入一个numpy合并用法，行合并
					    
					    
11  时序数据操作
    df.index = pd.date_range('2018/1/1', period=df.shape[0]) # 添加时间序列作为行名
    pd.to_datetime(df.a, format='%Y') # 将df的a列转化成datetime类型的年				     				    
    
    import datetime as dt
    dt.datetime(2015,1,1) # 通过datetime包定义数据时间
    dt.datetime.today() # 返回今天对应的时间
    
    s_update = pd.to_datetime(apps['Last Updated'])
    apps['Update_diff'] = s_update.max() - s_update     #求与最近的时间的时间差

12 绘图命令
    1. pandas内置绘图 
    df.plot.bar() # barplot, stacked=True, 堆叠
    df.plot.barh() # 绘制水平的barplot
    df.plot.hist(bins = 20) # 绘制直方图,单维度
    df.plot.box() # 对每列去看一些分布outlier
    df.plot.area() # 堆叠区域图
    df.plot.scatter(x='a', y='b') # 散点图
    df.plot.pie(subplots=True) # 绘制带图例的饼图
    
    2. matplotlib绘图的一些设置
     plt.figure(figsize=(12,8)) # 设置画布大小
     plt.xticks([2,4,6], [r'a',r'b',r'c']) # 设置坐标轴刻度
     plt.xlim(1,3) # 设置坐标位置
     plt.title() # 标题
     plt.xlabel('xxx', fontsize=18) # 绘制label
     plt.text(0.8, 0.9, 'xxx', color='k', fontsize=15) # 进行注解
     plt.grid(True) # 网格线
   
					    
    3. seaborn绘图
     sns.displot(x, kde=True, bins=20, rug=True, fit=stats.gamma) # histgram加密度线,样本分布情况, 拟合某些分布fit，适合单变量
     sns.kdeplot # 类似于上面的,kde是每个样本用正态分布画,如果样本多,高度就高,之后再做归一化
     sns.jointplot(x,y,data) # 绘制带有histgram以及散点图的图，两个变量
     sns.pairplot(df) # 直接绘制各个列之间的散点图以及对应的histgram，多个变量	
     
     #多图绘制1
     g = sns.PairGrik(df) # 各个列混合,产出n*n个格子
     g.map_diag(sns.kdeplot) # 对角线绘制
     g.map_offdiag(sns.kdeplot, cmap='Blues_d', n_levels=20) # 绘制对角线是kde密度图其他为等高线的图			    
					    
     #多图绘制2
     g = FaceGrid(row=[..],aspect=1.5, data=)           #这个类就是根据子集数据分别实例化对应的坐标轴，然后在坐标轴上绘制出对应的图形
     g.map(sns.boxplot, x, y, hue, hue_order=[], ...)
     
     #多图绘制3					    
     g = sns.PairGrid(data, x_vars=[], y_vars=[], aspect=0.5, size=3.5)
     g.map(sns.violinplot, palette='bright') # x_vars数量*y_vars数量个子图，然后每个子图都绘制violinplot
	
     #关联分析
     sns.lmplot(x, y, data) # 散点图+线性回归,95%置信区间,适用于连续值
		
     sns.residplot() # 残差图
     sns.barplot(x,y,hue,ci=None)  # 是否打开置信区间
     sns.stripplot(x, y, data, jitter =True) # 基于x为离散数据的,类似于散点图的boxplot
     sns.swarmplot(x, y, data) #  蜂群图，类似于小提琴图的点版				    
     sns.boxplot()
     sns.violinplot(bw) # 属于kde以及boxplot的组合，既看了单变量分布，也看了各变量之间的差异				    
     sns.violinplot(split=True， hue， inner='stick') # split将hue为两个类型的进行拼接绘制小提琴图，stick，每个样本绘制竖线
     sns.countplot(x, data) # 绘制离散变量数量分布，类似于value_counts()，类似于barplot但是使用的统计量是数量				    
     sns.pointplot(x, y, hue) # 查看离散变量x以及hue在离散变量y上的差别，使用均值，画点
     sns.factorplot(x, y, hue, col, data, kind='swarm') # 是一种泛化的绘图函数
     a.savefig('xx') # 进行图片存储 plt函数	
     
     #绘制热力图，通常都是绘制相关系数矩阵
     correlation = df[["xxx","yyy"]].corr()
     fig = plt.figure(figsize = (10, 10))
     sns.heatmap(correlation, vmax=.8, square=True, annot=True)
					    
					    
					    
